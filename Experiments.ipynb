{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Experiments.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "IbE43CXtc5ue"
      },
      "source": [
        "from google_drive_downloader import GoogleDriveDownloader as gdd\n",
        "import random, os, json, re, math, statistics\n",
        "import numpy as np\n",
        "from numpy import load\n",
        "from numpy import save\n",
        "import scipy as sp\n",
        "from scipy import io\n",
        "import cv2 as cv \n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.models import load_model\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, BatchNormalization, Conv2D, Dropout, Flatten, MaxPooling2D, Activation\n",
        "from sklearn.metrics import classification_report, plot_confusion_matrix\n",
        "from keras.utils.vis_utils import plot_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nj4JinDueoR5"
      },
      "source": [
        "# Mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EdTr4ZThesck"
      },
      "source": [
        "# Download dataset\n",
        "gdd.download_file_from_google_drive(file_id='15M2oVwpl1UwUgreZDXj1QlLY8pv0cdSl',\n",
        "                                    dest_path='./RetailDataset.zip',\n",
        "                                    unzip=True)\n",
        "os.remove('./RetailDataset.zip')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YvcS2QOqeuYC"
      },
      "source": [
        "### Load Data\n",
        "path = './drive/My Drive/MSc Project/'\n",
        "x_train_name = open(path+'x_train_name.txt').readlines()\n",
        "x_val_name = open(path+'x_val_name.txt').readlines()\n",
        "x_test_name = open(path+'x_test_name.txt').readlines()\n",
        "y_train = load(path+'y_train.npy')\n",
        "y_val = load(path+'y_val.npy')\n",
        "y_test = load(path+'y_test.npy')\n",
        "x_val = load(path+'x_val.npy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z76IZ8FIey7f"
      },
      "source": [
        "### Data Generator\n",
        "class InputData:\n",
        "  def __init__(self, img_folder_path, img_names, labels, batch_size):\n",
        "    '''\n",
        "    Args:\n",
        "      img_folder_path: path to image folder\n",
        "      img_names: all image names\n",
        "      labels: image labels\n",
        "\n",
        "    Output:\n",
        "      a tuple of two elements:\n",
        "        an array of an image batch and a list of corresponding labels\n",
        "    '''\n",
        "    self.lines = img_names\n",
        "    self.batch_size = batch_size\n",
        "    self.folder = img_folder_path\n",
        "    self.y = labels\n",
        "    self._index = 0\n",
        "    self._end = False\n",
        "\n",
        "  @property\n",
        "  def increment_index(self):\n",
        "    self._index += self.batch_size\n",
        "\n",
        "  def generate_batch(self):\n",
        "    old_index = self._index\n",
        "    self.increment_index\n",
        "    if self._index <= len(self.lines):\n",
        "      new_index = self._index\n",
        "    else:\n",
        "      new_index = len(self.lines)\n",
        "      self._end = True\n",
        "    input_shape = (new_index-old_index,256,256,3) # input image shape is (256,256,3)\n",
        "    result = np.zeros(input_shape)\n",
        "    counter = 0\n",
        "    for i in range(old_index,new_index):\n",
        "      img_path = self.folder+self.lines[i].strip()\n",
        "      crop_img = cv.imread(img_path)[172:1772,496:2096]/255 # crop image to (160,160)\n",
        "      result[counter] = cv.resize(crop_img, (256,256)) # resize image to (256,256)\n",
        "      counter += 1\n",
        "    return result, self.y[old_index:new_index]\n",
        "\n",
        "def generate_batches(image_paths,labels,batch_size,epoch):\n",
        "  for i in range(epoch):\n",
        "    batch = InputData('./train2019/',image_paths,labels,batch_size)\n",
        "    while batch._end==False:\n",
        "      yield batch.generate_batch()\n",
        "\n",
        "def partial_train(length,percentage):\n",
        "  return math.ceil(length*percentage)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wRnqE7DcfCXs"
      },
      "source": [
        "### CNN training\n",
        "def cnn_train_models(x_train_name, y_train, x_val, y_val, train_proportion,\n",
        "               batch_size, num_epochs, num_conv, num_full):\n",
        "  model = Sequential()\n",
        "  model.add(BatchNormalization())\n",
        "\n",
        "  if num_conv==1 or num_conv==2 or num_conv==3:\n",
        "    model.add(Conv2D(32, kernel_size=(7,7), dilation_rate=2, input_shape=(256,256,3))) # a dilation layer\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling2D(pool_size=(5,5)))\n",
        " \n",
        "  if num_conv==2 or num_conv==3:\n",
        "    model.add(Conv2D(32, kernel_size=(7,7)))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling2D(pool_size=(5,5),strides=(2,2)))\n",
        "\n",
        "  if num_conv==3:\n",
        "    model.add(Conv2D(64, kernel_size=(7,7)))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling2D(pool_size=(5,5)))\n",
        "    \n",
        "  model.add(Flatten()) # Flattening the 2D arrays for fully connected layers\n",
        "\n",
        "  model.add(Dense(128, activation=tf.nn.relu))\n",
        "  model.add(BatchNormalization())\n",
        "\n",
        "  model.add(Dense(17,activation=tf.nn.softmax))\n",
        "  opt = keras.optimizers.Adam(learning_rate=0.0001) # Adam optimiser\n",
        "  model.compile(loss='sparse_categorical_crossentropy', # since label is an integer in [0,16]\n",
        "                optimizer=opt, metrics=['accuracy'])\n",
        "\n",
        "  # Train model on dataset\n",
        "  partial_index = partial_train(len(x_train_name),train_proportion)\n",
        "  history = model.fit(generate_batches(x_train_name[:partial_index],y_train[:partial_index],batch_size,num_epochs),\n",
        "                      steps_per_epoch=math.ceil(partial_index/batch_size), verbose=1, epochs=num_epochs,\n",
        "                      validation_data=(x_val,y_val))\n",
        "                      # steps_per_epoch = number of training samples/number of batches\n",
        "  \n",
        "  # Save model\n",
        "  proportion = int(train_proportion*100)\n",
        "  model_name = 'CNN_'+str(proportion)+'_'+str(num_conv)+'conv_'+str(num_full)+'full_'+str(batch_size)+'batch'\n",
        "  model.save('./drive/My Drive/MSc Project/Models/'+model_name)\n",
        "  return model, history.history\n",
        "\n",
        "### CNN evaluation\n",
        "def cnn_test_models(model,x_test_name,y_test):\n",
        "  test_result = model.evaluate(generate_batches(x_test_name,y_test,128,1), verbose=1)\n",
        "  return test_result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p91Jt4ilhGVD"
      },
      "source": [
        "### Summarise training history\n",
        "def summarise_history(history, name):\n",
        "  fig, axs = plt.subplots(2,1,figsize=(6,12))\n",
        "  plt.xlabel('epoch', fontsize=16)\n",
        "\n",
        "  # summarise history for loss\n",
        "  ax = axs[0]\n",
        "  ax.set_ylim([0,3.0])\n",
        "  ax.plot(history['loss'])\n",
        "  ax.plot(history['val_loss'])\n",
        "  ax.set_title('training loss',fontsize=16)\n",
        "  ax.set_xticks(np.arange(0,11,2))\n",
        "  ax.legend(['train', 'validation'], loc='upper left')\n",
        "\n",
        "  # summarise history for accuracy\n",
        "  ax = axs[1]\n",
        "  ax.plot(history['accuracy'])\n",
        "  ax.plot(history['val_accuracy'])\n",
        "  ax.set_title('classification accuracy',fontsize=16)\n",
        "  ax.set_xticks(np.arange(0,11,2))\n",
        "  ax.set_yticks(np.arange(0,1.1,0.1))\n",
        "  ax.legend(['train', 'validation'], loc='upper left')\n",
        "\n",
        "  #plt.show()\n",
        "  plt.savefig(name+'.png', bbox_inches='tight')\n",
        "  plt.close(fig)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sg4FqmEcfire"
      },
      "source": [
        "### Experiment on data quantity\n",
        "\n",
        "# 20%\n",
        "model, model_history = cnn_train_models(x_train_name,y_train,x_val,y_val,train_proportion=0.2,\n",
        "                           batch_size=128,num_epochs=10,num_conv=1,num_full=2)\n",
        "print(model_history)\n",
        "summarise_history(model_history,'0.2_1conv')\n",
        "test_result = cnn_test_models(model,x_test_name,y_test)\n",
        "print(test_result)\n",
        "\n",
        "# 40%\n",
        "model, model_history = cnn_train_models(x_train_name,y_train,x_val,y_val,train_proportion=0.4,\n",
        "                           batch_size=128,num_epochs=10,num_conv=1,num_full=2)\n",
        "print(model_history)\n",
        "summarise_history(model_history,'0.4_1conv')\n",
        "test_result = cnn_test_models(model,x_test_name,y_test)\n",
        "print(test_result)\n",
        "\n",
        "# 60%\n",
        "model, model_history = cnn_train_models(x_train_name,y_train,x_val,y_val,train_proportion=0.6,\n",
        "                           batch_size=128,num_epochs=10,num_conv=1,num_full=2)\n",
        "print(model_history)\n",
        "summarise_history(model_history,'0.6_1conv')\n",
        "test_result = cnn_test_models(model,x_test_name,y_test)\n",
        "print(test_result)\n",
        "\n",
        "# 80%\n",
        "model, model_history = cnn_train_models(x_train_name,y_train,x_val,y_val,train_proportion=0.8,\n",
        "                           batch_size=128,num_epochs=10,num_conv=1,num_full=2)\n",
        "print(model_history)\n",
        "#summarise_history(model_history,'0.8_1conv') # this model diverges\n",
        "test_result = cnn_test_models(model,x_test_name,y_test)\n",
        "print(test_result)\n",
        "\n",
        "# 100%\n",
        "model, model_history = cnn_train_models(x_train_name,y_train,x_val,y_val,train_proportion=1.0,\n",
        "                           batch_size=128,num_epochs=10,num_conv=1,num_full=2)\n",
        "print(model_history)\n",
        "summarise_history(model_history,'1.0_1conv')\n",
        "test_result = cnn_test_models(model,x_test_name,y_test)\n",
        "print(test_result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0RwrIiVf3iZ"
      },
      "source": [
        "### Experiment on CNN depth\n",
        "\n",
        "# 1 convolutional layer, using 60% data\n",
        "# training has be done in previous experiment\n",
        "\n",
        "# 2 convolutional layers, using 60% data\n",
        "model, model_history = cnn_train_models(x_train_name,y_train,x_val,y_val,train_proportion=0.6,\n",
        "                           batch_size=128,num_epochs=10,num_conv=2,num_full=2)\n",
        "print(model_history)\n",
        "summarise_history(model_history,'0.6_2conv')\n",
        "test_result = cnn_test_models(model,x_test_name,y_test)\n",
        "print(test_result)\n",
        "\n",
        "# 3 convolutional layers, using 60% data\n",
        "model, model_history = cnn_train_models(x_train_name,y_train,x_val,y_val,train_proportion=0.6,\n",
        "                           batch_size=128,num_epochs=10,num_conv=3,num_full=2)\n",
        "print(model_history)\n",
        "summarise_history(model_history,'0.6_3conv')\n",
        "test_result = cnn_test_models(model,x_test_name,y_test)\n",
        "print(test_result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LgHeaBGPgDph"
      },
      "source": [
        "### The CNN that yields the optimal result in this research\n",
        "#100% data, 8 epochs\n",
        "model, model_history = cnn_train_models(x_train_name,y_train,x_val,y_val,train_proportion=1.0,\n",
        "                           batch_size=128,num_epochs=8,num_conv=1,num_full=2)\n",
        "print(model_history)\n",
        "test_result = cnn_test_models(model,x_test_name,y_test)\n",
        "print(test_result)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}